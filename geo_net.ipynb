{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82de84f-15b5-4647-8592-03499a12b338",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run mt_datagen.ipynb\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix as sparse\n",
    "import torch\n",
    "from torch.nn import Softmax as soft\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import TopKPooling\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import GCNConv, GravNetConv, PANConv, PANPooling, ASAPooling, SAGPooling, ChebConv, GraphConv,GATv2Conv\n",
    "from torch_geometric.nn import GatedGraphConv \n",
    "from torch_geometric.nn import ResGatedGraphConv\n",
    "from torch_geometric.nn import SAGEConv, GraphConv, GINConv, BatchNorm,  DynamicEdgeConv\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.datasets import TUDataset  \n",
    "import torch_geometric as tg\n",
    "from torch_geometric.data import DataLoader\n",
    "import random\n",
    "from torchsummary import summary\n",
    "print('GPU backend =', torch.cuda.is_available())\n",
    "#os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4d56a5-cac3-4823-89e2-0ce350841714",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model:\n",
    "    def __init__(self):\n",
    "        self.N = 1000\n",
    "        self.cut = 0.9\n",
    "        self.epoch = 50\n",
    "        self.batch_size = 200\n",
    "\n",
    "        self.batch_amount = 2000 # batch_amount = batches*CPU_threads\n",
    "        self.graph_type = \"ER\" #graph topology\n",
    "        self.CPU_threads = 8 #number of threads\n",
    "        self.nodes_N = 400 #number of simulated nodes\n",
    "        self.iterations = 50\n",
    "        self.infection_probability = 1/35\n",
    "        self.removal_probability = 1/40\n",
    "        self.latent_period = 1/30\n",
    "        self.p = 4 #mean number of edges per node\n",
    "        self.one_hot_features = True\n",
    "        self.two_d_labels = False\n",
    "        self.show_progress = False\n",
    "        self.types = 'SEIR'\n",
    "        self.filename = '' #leave empty for automatic generated\n",
    "        self.save = False\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    def generator(self):\n",
    "        # execute datagen from mt_datagen.ipynb\n",
    "        data = datagen(batch_amount = self.batch_amount, graph_type = self.graph_type,p = self.p/self.nodes_N , CPU_threads = self.CPU_threads, N = self.nodes_N, \n",
    "                iterations = self.iterations, infection_probability = self.infection_probability, removal_probability = self.removal_probability, \n",
    "                latent_period = self.latent_period, filename = self.filename, one_hot_features = self.one_hot_features, two_d_labels = self.two_d_labels, \n",
    "                show_progress = self.show_progress, types=self.types,save=self.save)\n",
    "        #generate data given the parameters\n",
    "        data.generate()\n",
    "        self.dataset = data.show()\n",
    "        sims = len(dataset)\n",
    "        trainingsset, testset = dataset[:int(sims*self.cut)], dataset[int(sims*self.cut):]\n",
    "        # pack simulations into mini-batch \n",
    "        self.trainingsset = DataLoader(trainingsset, batch_size=self.batch_size, shuffle=True, drop_last=False)\n",
    "        self.testset = DataLoader(testset, batch_size=1, shuffle=True, drop_last=False)\n",
    "        print('dataset ready for deployment.')\n",
    "\n",
    "    def train(self):\n",
    "        # train network, print accuracy at the end\n",
    "        self.model0 = self.Net1().to(self.device)\n",
    "        self.optimizer0 = torch.optim.Adam(self.model0.parameters(), lr=0.01)\n",
    "        self.loss0 = torch.nn.CrossEntropyLoss()\n",
    "        threshold = 0.0\n",
    "\n",
    "        self.train_network()\n",
    "\n",
    "        #del trainingsset\n",
    "\n",
    "        self.model0.eval()\n",
    "        #perm = list(range(len(testset)))\n",
    "        #random.shuffle(perm)\n",
    "        #testset = [testset[index] for index in perm]\n",
    "\n",
    "        top5 = 0\n",
    "        top1 = 0\n",
    "        all1 = 0\n",
    "        prog = 0\n",
    "        \n",
    "        print(\"testset:\")\n",
    "        for it, data in enumerate(self.testset):\n",
    "            test = self.model0(data.to(self.device)).detach().to('cpu').numpy()\n",
    "            max1 = np.argsort(test[:,1])[-5:]\n",
    "\n",
    "            if np.argmax(data.y.detach().to('cpu').numpy()) in max1:\n",
    "                top5+=1\n",
    "                if np.argmax(data.y.detach().to('cpu').numpy()) == max1[-1]:\n",
    "                    top1+=1\n",
    "\n",
    "            all1 +=1\n",
    "\n",
    "            if it/len(self.testset)*100>=prog:\n",
    "                print(prog, \"%\")\n",
    "                prog+=10\n",
    "\n",
    "        print(\"top 5 Accuracy: \", top5/all1*100, \"%\")\n",
    "        print(\"top 1 Accuracy: \", top1/all1*100, \"%\")\n",
    "        if self.save == True: torch.save(self.model0, 'model')\n",
    "            \n",
    "        print(\"trainingset (testing for overfitting):\")\n",
    "        for it, data in enumerate(self.trainingsset[1000]):\n",
    "            test = self.model0(data.to(self.device)).detach().to('cpu').numpy()\n",
    "            max1 = np.argsort(test[:,1])[-5:]\n",
    "\n",
    "            if np.argmax(data.y.detach().to('cpu').numpy()) in max1:\n",
    "                top5+=1\n",
    "                if np.argmax(data.y.detach().to('cpu').numpy()) == max1[-1]:\n",
    "                    top1+=1\n",
    "\n",
    "            all1 +=1\n",
    "\n",
    "            if it/10>=prog:\n",
    "                print(prog, \"%\")\n",
    "                prog+=10\n",
    "\n",
    "        print(\"top 5 Accuracy: \", top5/all1*100, \"%\")\n",
    "        print(\"top 1 Accuracy: \", top1/all1*100, \"%\")\n",
    "\n",
    "\n",
    "    # wrapper to measure execution time of function\n",
    "    def runtime(func): \n",
    "        def wrapper(*args, **kargs):\n",
    "            print(\"\\033[1mExecuting function\", func.__name__,\":\\n\\033[0m\")\n",
    "            start = t.time()\n",
    "            result = func(*args, **kargs)\n",
    "            print (\"\\033[1m\\nFinished execution of function {0} in {1:.5f} seconds.\\n\\033[0m\".format(func.__name__, (t.time()-start)))\n",
    "            return result\n",
    "        return wrapper\n",
    "\n",
    "    #onehot -> non-onehot\n",
    "    def from_one_hot(dataset):\n",
    "        prog = 0\n",
    "        l = len(dataset)\n",
    "        for it, data in enumerate(dataset):\n",
    "            if data.y.ndim != 2:\n",
    "                print(\"data {} does not have 2 dims... skipping\".format(it))\n",
    "            else:\n",
    "                data.y = data.y[:,1]\n",
    "        print(\"dataset now converted from one-hot\")\n",
    "    \n",
    "    # print status of current dataset\n",
    "    def model_status(self):\n",
    "        sus, exp, inf, rem = ([] for i in range(4))\n",
    "        for i, d in enumerate(self.dataset):\n",
    "            d = data[i]['x']\n",
    "            sus.append(np.count_nonzero(d[:,0]))\n",
    "            inf.append(np.count_nonzero(d[:,1]))\n",
    "            exp.append(np.count_nonzero(d[:,2]))\n",
    "            rem.append(np.count_nonzero(d[:,3]))\n",
    "        status = np.vstack((sus,exp,inf,rem))\n",
    "        maxrem = np.max(status[3])\n",
    "        status = np.mean(status, axis=1)\n",
    "        status = np.append(status,maxrem)\n",
    "        print(f'#Susceptible = {status[0]}\\n#Exposed = {status[1]}\\n#Infected = {status[2]}\\n#Removed = {status[3]}\\nmax # of removed = {status[4]}')\n",
    "    \n",
    "    # train function\n",
    "    def train_network(self):\n",
    "        self.model0.train()\n",
    "        for i in range(self.epoch):\n",
    "            loss_sum = np.array([])\n",
    "            for data in self.trainingsset:\n",
    "                data.to(self.device)\n",
    "                self.optimizer0.zero_grad()\n",
    "                out = self.model0(data)\n",
    "                y = data.y\n",
    "                loss = self.loss0(out, y.type(torch.long))\n",
    "                loss_sum = np.append(loss_sum,loss.item())\n",
    "                loss.backward()\n",
    "                self.optimizer0.step()\n",
    "            print(f'loss for epoch {i} = ',np.mean(loss_sum))\n",
    "\n",
    "        return self.model0, self.optimizer0\n",
    "\n",
    "\n",
    "    # different models we have tested. Final model is 'Net1'\n",
    "    class u_net_asap(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            normalize = True\n",
    "            size = 20\n",
    "            self.sagec1 = ResGatedGraphConv(4,size) #[20,200]\n",
    "            self.sagp1 = ASAPooling(size)\n",
    "            self.bn1 = BatchNorm(size)\n",
    "\n",
    "            self.sagec2 = ResGatedGraphConv(size,size)\n",
    "            self.sagp2 = ASAPooling(size)\n",
    "            self.bn2 = BatchNorm(size)\n",
    "            self.sagec3 = ResGatedGraphConv(size,size)\n",
    "\n",
    "\n",
    "            self.up1 = ResGatedGraphConv(size,size*2)\n",
    "\n",
    "            self.up2 = ResGatedGraphConv(size,size*2)\n",
    "\n",
    "            self.sagec7 = ResGatedGraphConv(size,size)\n",
    "            self.classifier = Linear(size,2)\n",
    "\n",
    "\n",
    "\n",
    "        def forward(self, data):\n",
    "            p = 0.1\n",
    "            x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "            x = self.sagec1(x, edge_index) #first layer sage\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=p)\n",
    "            #first downsampling layer\n",
    "            x1, edge_index1, _, batch1, _ = self.sagp1(x, edge_index, batch=batch)\n",
    "            x1 = self.sagec2(x1,edge_index1) #simple u-net\n",
    "            x1 = self.bn1(x1)\n",
    "            x1 = F.relu(x1)\n",
    "            x1 = F.dropout(x1, p=p)\n",
    "            #second downsampling layer\n",
    "            x2, edge_index2, _, batch2, _ = self.sagp1(x1, edge_index1, batch=batch1)\n",
    "            x2 = self.sagec2(x2,edge_index2) #simple u-net\n",
    "            x2 = self.bn2(x2)\n",
    "            x2 = F.relu(x2)\n",
    "            x2 = F.dropout(x2, p=p)\n",
    "            #first upscaling layer\n",
    "            x2 = self.up1(x2, edge_index2)\n",
    "            x2 = F.relu(x2)\n",
    "            x1 = x1+x2.reshape(x1.shape)\n",
    "            x1 = F.dropout(x1, p=p)\n",
    "            #second upscaling layer\n",
    "            x1 = self.up2(x1, edge_index1)\n",
    "            x1 = F.relu(x1)\n",
    "            x = x+x1.reshape(x.shape)\n",
    "            x = F.dropout(x, p=p)\n",
    "            #last concetenated layer\n",
    "            x = self.sagec3(x,edge_index) #third layer sage\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=p)\n",
    "            #classification layer                 \n",
    "            x = self.classifier(x)\n",
    "            x = F.log_softmax(x,dim=1)\n",
    "            return x\n",
    "\n",
    "    class res_gat_net(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.sconv1 = ResGatedGraphConv(4, 20)\n",
    "            self.resgatenn = ResGatedGraphConv(20, 20)\n",
    "            self.sconv2 = ResGatedGraphConv(20, 20)\n",
    "            self.sconv3 = ResGatedGraphConv(20, 20)\n",
    "            self.classifier1 = Linear(20,2)\n",
    "\n",
    "\n",
    "        def forward(self, data):\n",
    "            p = 0.1\n",
    "            x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "            x = self.sconv1(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training, p=p)\n",
    "            x = self.resgatenn(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training, p=p)\n",
    "            x = self.sconv2(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training, p=p)\n",
    "            x = self.sconv3(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training, p=p)\n",
    "            x = self.classifier1(x)\n",
    "            x = F.log_softmax(x,dim=1)\n",
    "            return x\n",
    "\n",
    "\n",
    "    class gatv2_conv(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.sconv1 = GATv2Conv(4, 20)\n",
    "            self.resgatenn = GATv2Conv(20, 20)\n",
    "            self.sconv2 = GATv2Conv(20, 20)\n",
    "            self.sconv3 = GATv2Conv(20, 20)\n",
    "            self.classifier1 = Linear(20,2)\n",
    "\n",
    "\n",
    "        def forward(self, data):\n",
    "            p = 0.1\n",
    "            x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "            x = self.sconv1(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training, p=p)\n",
    "            x = self.resgatenn(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training, p=p)\n",
    "            x = self.sconv2(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training, p=p)\n",
    "            x = self.sconv3(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training, p=p)\n",
    "            x = self.classifier1(x)\n",
    "            x = F.log_softmax(x,dim=1)\n",
    "            return x\n",
    "\n",
    "    class pure_sage(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.sconv1 = SAGEConv(4, 20)\n",
    "            self.resgatenn = SAGEConv(20, 20)\n",
    "            self.sconv2 = SAGEConv(20, 20)\n",
    "            self.sconv3 = SAGEConv(20, 20)\n",
    "            self.classifier1 = Linear(20,2)\n",
    "\n",
    "\n",
    "        def forward(self, data):\n",
    "            p = 0.1\n",
    "            x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "            x = self.sconv1(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training, p=p)\n",
    "            x = self.resgatenn(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training, p=p)\n",
    "            x = self.sconv2(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training, p=p)\n",
    "            x = self.sconv3(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training, p=p)\n",
    "            x = self.classifier1(x)\n",
    "            x = F.log_softmax(x,dim=1)\n",
    "            return x\n",
    "\n",
    "    class graph_conv(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.sconv1 = GraphConv(4, 20)\n",
    "            self.resgatenn = GraphConv(20, 20)\n",
    "            self.sconv2 = GraphConv(20, 20)\n",
    "            self.sconv3 = GraphConv(20, 20)\n",
    "            self.classifier1 = Linear(20,2)\n",
    "\n",
    "\n",
    "        def forward(self, data):\n",
    "            p = 0.1\n",
    "            x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "            x = self.sconv1(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training, p=p)\n",
    "            x = self.resgatenn(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training, p=p)\n",
    "            x = self.sconv2(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training, p=p)\n",
    "            x = self.sconv3(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training, p=p)\n",
    "            x = self.classifier1(x)\n",
    "            x = F.log_softmax(x,dim=1)\n",
    "            return x\n",
    "\n",
    "    class chebby_net(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            k = 3\n",
    "            self.sconv1 = ChebConv(4, 20,k)\n",
    "            self.resgatenn = ChebConv(20, 20,k)\n",
    "            self.sconv2 = ChebConv(20, 20,k)\n",
    "            self.sconv3 = ChebConv(20, 20,k)\n",
    "            self.classifier1 = Linear(20,2)\n",
    "\n",
    "\n",
    "        def forward(self, data):\n",
    "            p = 0.1\n",
    "            x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "            x = self.sconv1(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training, p=p)\n",
    "            x = self.resgatenn(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training, p=p)\n",
    "            x = self.sconv2(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training, p=p)\n",
    "            x = self.sconv3(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training, p=p)\n",
    "            x = self.classifier1(x)\n",
    "            x = F.log_softmax(x)\n",
    "            return x\n",
    "\n",
    "    #define Net\n",
    "    class sage_res(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.sconv1 = SAGEConv(4, 20)\n",
    "            self.resgatenn = ResGatedGraphConv(20, 20)\n",
    "            self.sconv2 = SAGEConv(20, 20)\n",
    "            self.sconv3 = SAGEConv(20, 20)\n",
    "            self.classifier1 = Linear(20,2)\n",
    "\n",
    "\n",
    "        def forward(self, data):\n",
    "            p = 0.1\n",
    "            x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "            x = self.sconv1(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training, p=p)\n",
    "            x = self.resgatenn(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training, p=p)\n",
    "            x = self.sconv2(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training, p=p)\n",
    "            x = self.sconv3(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training, p=p)\n",
    "            x = self.classifier1(x)\n",
    "            x = F.log_softmax(x,dim=1)\n",
    "            return x\n",
    "    #define Net\n",
    "    class gcn(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.sconv1 = GCNConv(4, 20)\n",
    "            self.resgatenn = GCNConv(20, 20)\n",
    "            self.sconv2 = GCNConv(20, 20)\n",
    "            self.sconv3 = GCNConv(20, 20)\n",
    "            self.classifier1 = Linear(20,2)\n",
    "\n",
    "\n",
    "        def forward(self, data):\n",
    "            p = 0.1\n",
    "            x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "            x = self.sconv1(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training, p=p)\n",
    "            x = self.resgatenn(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training, p=p)\n",
    "            x = self.sconv2(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training, p=p)\n",
    "            x = self.sconv3(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training, p=p)\n",
    "            x = self.classifier1(x)\n",
    "            x = F.log_softmax(x,dim=1)\n",
    "            return x\n",
    "\n",
    "    class Net1(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.sconv1 = ChebConv(1, 20,3)\n",
    "            self.resgatenn = ChebConv(20, 20,3)\n",
    "            self.sconv2 = ResGatedGraphConv(20, 20)\n",
    "            self.sconv3 = ResGatedGraphConv(20, 20)\n",
    "            self.classifier1 = Linear(20,2)\n",
    "\n",
    "\n",
    "        def forward(self, data):\n",
    "            p = 0.1\n",
    "            x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "            x = self.sconv1(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training, p=p)\n",
    "            x = self.resgatenn(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training, p=p)\n",
    "            x = self.sconv2(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training, p=p)\n",
    "            x = self.sconv3(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training, p=p)\n",
    "            x = self.classifier1(x)\n",
    "            x = F.log_softmax(x,dim=1)\n",
    "            return x\n",
    "\n",
    "    #define Net\n",
    "    class simple_u(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            normalize = True\n",
    "            self.sagec1 = SAGEConv(4,20, normalize=normalize) #[20,200]\n",
    "            self.sagp1 = SAGPooling(20)\n",
    "\n",
    "            self.sagec2 = SAGEConv(20,40, normalize=normalize)\n",
    "\n",
    "            self.sagec3 = SAGEConv(20,20, normalize=normalize)\n",
    "            self.classifier = Linear(20,2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        def forward(self, data):\n",
    "            p = 0.1\n",
    "            x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "            x = self.sagec1(x, edge_index) #first layer sage\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=p)\n",
    "\n",
    "\n",
    "            x1, edge_index1, weights1, batch1, perm,_ = self.sagp1(x, edge_index,batch=batch)\n",
    "            x1 = self.sagec2(x1,edge_index1) #simple u-net\n",
    "            x1 = F.relu(x1)\n",
    "            x1 = F.dropout(x1, p=p)\n",
    "            x = x+x1.reshape(x.shape) #concetenate U with first layer\n",
    "            x = self.sagec3(x,edge_index) #third layer sage\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=p)\n",
    "\n",
    "            x = self.classifier(x)\n",
    "            x = F.log_softmax(x,dim=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            return x\n",
    "\n",
    "\n",
    "    #define Net\n",
    "    class u_net(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            normalize = True\n",
    "            size = 20\n",
    "            self.sagec1 = SAGEConv(4,size, normalize=normalize) #[20,200]\n",
    "            self.sagp1 = SAGPooling(size)\n",
    "            self.bn1 = BatchNorm(size)\n",
    "\n",
    "            self.sagec2 = SAGEConv(size,size, normalize=normalize)\n",
    "            self.sagp2 = SAGPooling(size)\n",
    "            self.bn2 = BatchNorm(size)\n",
    "\n",
    "            self.sagec3 = SAGEConv(size,size, normalize=normalize)\n",
    "            self.sagp3 = SAGPooling(size)\n",
    "            self.bn3 = BatchNorm(size)\n",
    "\n",
    "            self.up1 = SAGEConv(size,size*2, normalize=normalize)\n",
    "\n",
    "            self.up2 = SAGEConv(size,size*2, normalize=normalize)\n",
    "\n",
    "            self.sagec7 = SAGEConv(size,size, normalize=normalize)\n",
    "            self.classifier = Linear(size,2)\n",
    "\n",
    "\n",
    "\n",
    "        def forward(self, data):\n",
    "            p = 0.1\n",
    "            x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "            x = self.sagec1(x, edge_index) #first layer sage\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=p)\n",
    "            #first downsampling layer\n",
    "            x1, edge_index1, weights1, batch1, perm1,_ = self.sagp1(x, edge_index, batch=batch)\n",
    "            x1 = self.sagec2(x1,edge_index1) #simple u-net\n",
    "            x1 = self.bn1(x1)\n",
    "            x1 = F.relu(x1)\n",
    "            x1 = F.dropout(x1, p=p)\n",
    "            #second downsampling layer\n",
    "            x2, edge_index2, weights2, batch2, perm2,_ = self.sagp1(x1, edge_index1, batch=batch1)\n",
    "            x2 = self.sagec2(x2,edge_index2) #simple u-net\n",
    "            x2 = self.bn2(x2)\n",
    "            x2 = F.relu(x2)\n",
    "            x2 = F.dropout(x2, p=p)\n",
    "            #first upscaling layer\n",
    "            x2 = self.up1(x2, edge_index2)\n",
    "            x2 = F.relu(x2)\n",
    "            x1 = x1+x2.reshape(x1.shape)\n",
    "            x1 = F.dropout(x1, p=p)\n",
    "            #second upscaling layer\n",
    "            x1 = self.up2(x1, edge_index1)\n",
    "            x1 = F.relu(x1)\n",
    "            x = x+x1.reshape(x.shape)\n",
    "            x = F.dropout(x, p=p)\n",
    "            #last concetenated layer\n",
    "            x = self.sagec3(x,edge_index) #third layer sage\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=p)\n",
    "            #classification layer                 \n",
    "            x = self.classifier(x)\n",
    "            x = F.log_softmax(x,dim=1)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4becb6b6-c98b-446d-b695-cbd71d2fc685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative implementation of pytorch_geometric.data.DataLoader with memory handling\n",
    "class classDataLoader:\n",
    "    def __init__(self, dataset, batchsize):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.batchsize = batchsize\n",
    "    \n",
    "    def __iter__(self):\n",
    "        self.index=0\n",
    "        self.batch = DataLoader(self.dataset, self.batchsize)\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.index >= batchsize-1:\n",
    "            raise StopIteration\n",
    "        return self.batch[self.index]\n",
    "\n",
    "    def batch(self):\n",
    "        return DataLoader(self.dataset, self.batchsize)\n",
    "    \n",
    " # \"backend\" implementation of DataLoader to create actual batches\n",
    "def DataLoader(dataset, batchsize):\n",
    "    batch_list = []\n",
    "    bat = Batch()\n",
    "    np.random.shuffle(dataset)\n",
    "    \n",
    "    N = len(dataset)\n",
    "    full_batches = int(N/batchsize)\n",
    "    extra = full_batches*batchsize\n",
    "    \n",
    "    for batch_number in range(full_batches):\n",
    "        bat = Batch()\n",
    "        data_list = dataset[batchsize*batch_number:batchsize*(batch_number+1)]\n",
    "        batch_list.append(bat.from_data_list(data_list))\n",
    "    \n",
    "    if extra!=N:\n",
    "        extra_data_list = dataset[extra:]\n",
    "        batch_list.append(bat.from_data_list(extra_data_list))\n",
    "    \n",
    "    return batch_list\n",
    "\n",
    "# returns dataset from list of batches (dtype Batch -> dtype Data)\n",
    "\n",
    "def DataUnLoader(batch_list):\n",
    "    dataset = []\n",
    "    batch_amount = len(batch_list)\n",
    "    \n",
    "    for batch_index in range(batch_amount):\n",
    "        dataset = dataset + batch_list[batch_index].to_data_list()\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba8f602-f1bd-4995-9dd6-ab41adc1aafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model()\n",
    "model.generator()\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0d3afd-eaf8-4934-a552-2b33dc13a2c2",
   "metadata": {},
   "source": [
    "# Aditional Data Manipulation and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669284be-dab9-4008-8787-dbf3ce237253",
   "metadata": {},
   "outputs": [],
   "source": [
    "def x_to_one_hot(dataset, features=4):\n",
    "    for it, data in enumerate(dataset):\n",
    "        one_hot = torch.zeros((data.x.shape[0], features))\n",
    "        one_hot[np.arange(data.x.shape[0], dtype=int), data.x[:,0].long()]= 1\n",
    "        data.x = one_hot.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ff0fdf-d8c3-4a1a-b9c5-101767a2543b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print histogram of one hot data distribution\n",
    "def datahist(dataset, SEIR=True):\n",
    "    susceptible = np.zeros(400)\n",
    "    infected = np.zeros(400)\n",
    "    removed = np.zeros(400)\n",
    "    \n",
    "    s, e, i, r = [], [], [], []\n",
    "    \n",
    "    if SEIR:\n",
    "        for data in dataset:\n",
    "            s.append(int(np.sum(data.x.cpu().numpy()[:,0])))\n",
    "            e.append(int(np.sum(data.x.cpu().numpy()[:,1])))\n",
    "            i.append(int(np.sum(data.x.cpu().numpy()[:,2])))\n",
    "            r.append(int(np.sum(data.x.cpu().numpy()[:,3])))\n",
    "        \n",
    "        plt.title(\"susceptible\")\n",
    "        plt.hist(s, bins=70)\n",
    "        #plt.savefig(\"WS1\")\n",
    "        plt.show()\n",
    "        plt.title(\"exposed\")\n",
    "        plt.hist(e, bins=70)\n",
    "        #plt.savefig(\"WS2\")\n",
    "        plt.show()\n",
    "        plt.title(\"infected\")\n",
    "        plt.hist(i, bins=70)\n",
    "        #plt.savefig(\"WS3\")\n",
    "        plt.show()\n",
    "        plt.title(\"removed\")\n",
    "        plt.hist(r, bins=70)\n",
    "        #plt.savefig(\"WS4\")\n",
    "        plt.show()\n",
    "    \n",
    "    else:\n",
    "        for data in dataset:\n",
    "            s.append(int(np.sum(data.x.cpu().numpy()[:,0])))\n",
    "            i.append(int(np.sum(data.x.cpu().numpy()[:,1])))\n",
    "            r.append(int(np.sum(data.x.cpu().numpy()[:,2])))\n",
    "        \n",
    "        plt.title(\"susceptible\")\n",
    "        plt.hist(s, bins=70)\n",
    "        #plt.savefig(\"er_sir1.jpg\")\n",
    "        plt.show()\n",
    "        plt.title(\"infected\")\n",
    "        plt.hist(i, bins=70)\n",
    "        #plt.savefig(\"er_sir2.jpg\")\n",
    "        plt.show()\n",
    "        plt.title(\"removed\")\n",
    "        plt.hist(r, bins=70)\n",
    "        #plt.savefig(\"er_sir3.jpg\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee52096-4c69-432f-8f55-38a995eb9b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function can be used to remove graphs where only one node is not susceptible\n",
    "def onenodefilter(dataset, total=400):\n",
    "    newdataset = dataset\n",
    "    simple = []\n",
    "    \n",
    "    for i, data in enumerate(dataset):\n",
    "         if torch.sum(data.x[:,0]).item() == total-1:\n",
    "            simple.append(i)\n",
    "\n",
    "    for s in simple[::-1]:\n",
    "        del newdataset[s]\n",
    "    print(\"len: \", len(newdataset))\n",
    "    return newdataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bb8eac-0972-458d-a676-e9614b20b149",
   "metadata": {},
   "source": [
    "# DMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2261d2-83ae-46fc-a068-66beceb69a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run DMP.ipynb\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b81417b-5c9a-4d54-a417-5ef6a1142186",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_amount = 100\n",
    "# Generate Data using SEIRdata\n",
    "graph_model = SIRdata(batches = batch_amount,show_progress=True, graph_type = \"ER\", iterations = 50, N=50 ,p=6/100, beta=1/20, gamma=1/50)\n",
    "sirData = graph_model.generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28a4257-a522-490e-b026-010ca98ae102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to dtype data\n",
    "sir_dataset = []\n",
    "for graph_index in range(batch_amount):\n",
    "    sir_dataset.append(Data(edge_index = sirData[graph_index][1], x = sirData[graph_index][2], y = sirData[graph_index][3]))\n",
    "    sir_dataset[-1] = sir_dataset[-1].to(device)\n",
    "del sirData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df36bea7-a22a-4280-8a46-8a1176e7c82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dmp\n",
    "top10 = 0\n",
    "top5 = 0\n",
    "top1 = 0\n",
    "total = 0\n",
    "prog = 0\n",
    "\n",
    "tester = dmp_layer(lamb=1/40,mu=1/100) \n",
    "\n",
    "for it, data in enumerate(sir_dataset):\n",
    "    total +=1\n",
    "    edge_index = data.edge_index\n",
    "    snapshot = data.x\n",
    "    \n",
    "    prediction = tester.predict(edge_index, snapshot=snapshot, time_steps=100)\n",
    "    \n",
    "    correct = data.y.argmax().cpu().numpy()\n",
    "    if correct in prediction.argsort()[-10:]:\n",
    "        top10+=1\n",
    "        if correct in prediction.argsort()[-5:]:\n",
    "            top5+=1\n",
    "            if correct == prediction.argmax():\n",
    "                top1+=1\n",
    "\n",
    "    if it/len(sir_dataset)*100>prog:\n",
    "        print(\"prog\", prog, \"%\")\n",
    "        prog+=1\n",
    "print(\"top10 accuracy: \", top10/total*100, \"%\")\n",
    "print(\"top5 accuracy: \", top5/total*100, \"%\")\n",
    "print(\"top1 accuracy: \", top1/total*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76475670-4ba5-435d-87f7-1dc8edc975b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
