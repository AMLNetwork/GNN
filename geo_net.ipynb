{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e82de84f-15b5-4647-8592-03499a12b338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU backend = True\n"
     ]
    }
   ],
   "source": [
    "%run mt_datagen.ipynb\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix as sparse\n",
    "import torch\n",
    "from torch.nn import Softmax as soft\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import TopKPooling\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import GCNConv, GravNetConv, PANConv, PANPooling, ASAPooling, SAGPooling, ChebConv, GraphConv,GATv2Conv\n",
    "from torch_geometric.nn import GatedGraphConv \n",
    "from torch_geometric.nn import ResGatedGraphConv\n",
    "from torch_geometric.nn import SAGEConv, GraphConv, GINConv, BatchNorm,  DynamicEdgeConv\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.datasets import TUDataset  \n",
    "import torch_geometric as tg\n",
    "from torch_geometric.data import DataLoader\n",
    "import random\n",
    "from torchsummary import summary\n",
    "print('GPU backend =', torch.cuda.is_available())\n",
    "#os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c4d56a5-cac3-4823-89e2-0ce350841714",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model:\n",
    "    def __init__(self):\n",
    "        self.N = 1000\n",
    "        self.cut = 0.9\n",
    "        self.epoch = 50\n",
    "        self.batch_size = 200\n",
    "\n",
    "        self.batch_amount = 2000 # batch_amount = batches*CPU_threads\n",
    "        self.graph_type = \"ER\" #graph topology\n",
    "        self.CPU_threads = 8 #number of threads\n",
    "        self.nodes_N = 400 #number of simulated nodes\n",
    "        self.iterations = 50\n",
    "        self.infection_probability = 1/35\n",
    "        self.removal_probability = 1/40\n",
    "        self.latent_period = 1/30\n",
    "        self.p = 4 #mean number of edges per node\n",
    "        self.one_hot_features = True\n",
    "        self.two_d_labels = False\n",
    "        self.show_progress = False\n",
    "        self.types = 'SEIR'\n",
    "        self.filename = '' #leave empty for automatic generated\n",
    "        self.save = False\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    def generator(self):\n",
    "        # execute datagen from mt_datagen.ipynb\n",
    "        data = datagen(batch_amount = self.batch_amount, graph_type = self.graph_type,p = self.p/self.nodes_N , CPU_threads = self.CPU_threads, N = self.nodes_N, \n",
    "                iterations = self.iterations, infection_probability = self.infection_probability, removal_probability = self.removal_probability, \n",
    "                latent_period = self.latent_period, filename = self.filename, one_hot_features = self.one_hot_features, two_d_labels = self.two_d_labels, \n",
    "                show_progress = self.show_progress, types=self.types,save=self.save)\n",
    "        #generate data given the parameters\n",
    "        data.generate()\n",
    "        self.dataset = data.show()\n",
    "        sims = len(dataset)\n",
    "        trainingsset, testset = dataset[:int(sims*self.cut)], dataset[int(sims*self.cut):]\n",
    "        # pack simulations into mini-batch \n",
    "        self.trainingsset = DataLoader(trainingsset, batch_size=self.batch_size, shuffle=True, drop_last=False)\n",
    "        self.testset = DataLoader(testset, batch_size=1, shuffle=True, drop_last=False)\n",
    "        print('dataset ready for deployment.')\n",
    "\n",
    "    def train(self):\n",
    "        # train network, print accuracy at the end\n",
    "        self.model0 = self.Net1().to(self.device)\n",
    "        self.optimizer0 = torch.optim.Adam(self.model0.parameters(), lr=0.01)\n",
    "        self.loss0 = torch.nn.CrossEntropyLoss()\n",
    "        threshold = 0.0\n",
    "\n",
    "        self.train_network()\n",
    "\n",
    "        #del trainingsset\n",
    "\n",
    "        self.model0.eval()\n",
    "        #perm = list(range(len(testset)))\n",
    "        #random.shuffle(perm)\n",
    "        #testset = [testset[index] for index in perm]\n",
    "\n",
    "        top5 = 0\n",
    "        top1 = 0\n",
    "        all1 = 0\n",
    "        prog = 0\n",
    "        \n",
    "        print(\"testset:\")\n",
    "        for it, data in enumerate(self.testset):\n",
    "            test = self.model0(data.to(self.device)).detach().to('cpu').numpy()\n",
    "            max1 = np.argsort(test[:,1])[-5:]\n",
    "\n",
    "            if np.argmax(data.y.detach().to('cpu').numpy()) in max1:\n",
    "                top5+=1\n",
    "                if np.argmax(data.y.detach().to('cpu').numpy()) == max1[-1]:\n",
    "                    top1+=1\n",
    "\n",
    "            all1 +=1\n",
    "\n",
    "            if it/len(self.testset)*100>=prog:\n",
    "                print(prog, \"%\")\n",
    "                prog+=10\n",
    "\n",
    "        print(\"top 5 Accuracy: \", top5/all1*100, \"%\")\n",
    "        print(\"top 1 Accuracy: \", top1/all1*100, \"%\")\n",
    "        if self.save == True: torch.save(self.model0, 'model')\n",
    "            \n",
    "        print(\"trainingset (testing for overfitting):\")\n",
    "        for it, data in enumerate(self.trainingsset[1000]):\n",
    "            test = self.model0(data.to(self.device)).detach().to('cpu').numpy()\n",
    "            max1 = np.argsort(test[:,1])[-5:]\n",
    "\n",
    "            if np.argmax(data.y.detach().to('cpu').numpy()) in max1:\n",
    "                top5+=1\n",
    "                if np.argmax(data.y.detach().to('cpu').numpy()) == max1[-1]:\n",
    "                    top1+=1\n",
    "\n",
    "            all1 +=1\n",
    "\n",
    "            if it/10>=prog:\n",
    "                print(prog, \"%\")\n",
    "                prog+=10\n",
    "\n",
    "        print(\"top 5 Accuracy: \", top5/all1*100, \"%\")\n",
    "        print(\"top 1 Accuracy: \", top1/all1*100, \"%\")\n",
    "\n",
    "\n",
    "    # wrapper to measure execution time of function\n",
    "    def runtime(func): \n",
    "        def wrapper(*args, **kargs):\n",
    "            print(\"\\033[1mExecuting function\", func.__name__,\":\\n\\033[0m\")\n",
    "            start = t.time()\n",
    "            result = func(*args, **kargs)\n",
    "            print (\"\\033[1m\\nFinished execution of function {0} in {1:.5f} seconds.\\n\\033[0m\".format(func.__name__, (t.time()-start)))\n",
    "            return result\n",
    "        return wrapper\n",
    "\n",
    "    #onehot -> non-onehot\n",
    "    def from_one_hot(dataset):\n",
    "        prog = 0\n",
    "        l = len(dataset)\n",
    "        for it, data in enumerate(dataset):\n",
    "            if data.y.ndim != 2:\n",
    "                print(\"data {} does not have 2 dims... skipping\".format(it))\n",
    "            else:\n",
    "                data.y = data.y[:,1]\n",
    "        print(\"dataset now converted from one-hot\")\n",
    "    \n",
    "    # print status of current dataset\n",
    "    def model_status(self):\n",
    "        sus, exp, inf, rem = ([] for i in range(4))\n",
    "        for i, d in enumerate(self.dataset):\n",
    "            d = data[i]['x']\n",
    "            sus.append(np.count_nonzero(d[:,0]))\n",
    "            inf.append(np.count_nonzero(d[:,1]))\n",
    "            exp.append(np.count_nonzero(d[:,2]))\n",
    "            rem.append(np.count_nonzero(d[:,3]))\n",
    "        status = np.vstack((sus,exp,inf,rem))\n",
    "        maxrem = np.max(status[3])\n",
    "        status = np.mean(status, axis=1)\n",
    "        status = np.append(status,maxrem)\n",
    "        print(f'#Susceptible = {status[0]}\\n#Exposed = {status[1]}\\n#Infected = {status[2]}\\n#Removed = {status[3]}\\nmax # of removed = {status[4]}')\n",
    "    \n",
    "    # train function\n",
    "    def train_network(self):\n",
    "        self.model0.train()\n",
    "        for i in range(self.epoch):\n",
    "            loss_sum = np.array([])\n",
    "            for data in self.trainingsset:\n",
    "                data.to(self.device)\n",
    "                self.optimizer0.zero_grad()\n",
    "                out = self.model0(data)\n",
    "                y = data.y\n",
    "                loss = self.loss0(out, y.type(torch.long))\n",
    "                loss_sum = np.append(loss_sum,loss.item())\n",
    "                loss.backward()\n",
    "                self.optimizer0.step()\n",
    "            print(f'loss for epoch {i} = ',np.mean(loss_sum))\n",
    "\n",
    "        return self.model0, self.optimizer0\n",
    "\n",
    "\n",
    "    # different models we have tested. Final model is 'Net1'\n",
    "    class u_net_asap(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            normalize = True\n",
    "            size = 20\n",
    "            self.sagec1 = ResGatedGraphConv(4,size) #[20,200]\n",
    "            self.sagp1 = ASAPooling(size)\n",
    "            self.bn1 = BatchNorm(size)\n",
    "\n",
    "            self.sagec2 = ResGatedGraphConv(size,size)\n",
    "            self.sagp2 = ASAPooling(size)\n",
    "            self.bn2 = BatchNorm(size)\n",
    "            self.sagec3 = ResGatedGraphConv(size,size)\n",
    "\n",
    "\n",
    "            self.up1 = ResGatedGraphConv(size,size*2)\n",
    "\n",
    "            self.up2 = ResGatedGraphConv(size,size*2)\n",
    "\n",
    "            self.sagec7 = ResGatedGraphConv(size,size)\n",
    "            self.classifier = Linear(size,2)\n",
    "\n",
    "\n",
    "\n",
    "        def forward(self, data):\n",
    "            p = 0.1\n",
    "            x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "            x = self.sagec1(x, edge_index) #first layer sage\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=p)\n",
    "            #first downsampling layer\n",
    "            x1, edge_index1, _, batch1, _ = self.sagp1(x, edge_index, batch=batch)\n",
    "            x1 = self.sagec2(x1,edge_index1) #simple u-net\n",
    "            x1 = self.bn1(x1)\n",
    "            x1 = F.relu(x1)\n",
    "            x1 = F.dropout(x1, p=p)\n",
    "            #second downsampling layer\n",
    "            x2, edge_index2, _, batch2, _ = self.sagp1(x1, edge_index1, batch=batch1)\n",
    "            x2 = self.sagec2(x2,edge_index2) #simple u-net\n",
    "            x2 = self.bn2(x2)\n",
    "            x2 = F.relu(x2)\n",
    "            x2 = F.dropout(x2, p=p)\n",
    "            #first upscaling layer\n",
    "            x2 = self.up1(x2, edge_index2)\n",
    "            x2 = F.relu(x2)\n",
    "            x1 = x1+x2.reshape(x1.shape)\n",
    "            x1 = F.dropout(x1, p=p)\n",
    "            #second upscaling layer\n",
    "            x1 = self.up2(x1, edge_index1)\n",
    "            x1 = F.relu(x1)\n",
    "            x = x+x1.reshape(x.shape)\n",
    "            x = F.dropout(x, p=p)\n",
    "            #last concetenated layer\n",
    "            x = self.sagec3(x,edge_index) #third layer sage\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=p)\n",
    "            #classification layer                 \n",
    "            x = self.classifier(x)\n",
    "            x = F.log_softmax(x,dim=1)\n",
    "            return x\n",
    "\n",
    "    class res_gat_net(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.sconv1 = ResGatedGraphConv(4, 20)\n",
    "            self.resgatenn = ResGatedGraphConv(20, 20)\n",
    "            self.sconv2 = ResGatedGraphConv(20, 20)\n",
    "            self.sconv3 = ResGatedGraphConv(20, 20)\n",
    "            self.classifier1 = Linear(20,2)\n",
    "\n",
    "\n",
    "        def forward(self, data):\n",
    "            p = 0.1\n",
    "            x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "            x = self.sconv1(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training, p=p)\n",
    "            x = self.resgatenn(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training, p=p)\n",
    "            x = self.sconv2(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training, p=p)\n",
    "            x = self.sconv3(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training, p=p)\n",
    "            x = self.classifier1(x)\n",
    "            x = F.log_softmax(x,dim=1)\n",
    "            return x\n",
    "\n",
    "\n",
    "    class gatv2_conv(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.sconv1 = GATv2Conv(4, 20)\n",
    "            self.resgatenn = GATv2Conv(20, 20)\n",
    "            self.sconv2 = GATv2Conv(20, 20)\n",
    "            self.sconv3 = GATv2Conv(20, 20)\n",
    "            self.classifier1 = Linear(20,2)\n",
    "\n",
    "\n",
    "        def forward(self, data):\n",
    "            p = 0.1\n",
    "            x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "            x = self.sconv1(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training, p=p)\n",
    "            x = self.resgatenn(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training, p=p)\n",
    "            x = self.sconv2(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training, p=p)\n",
    "            x = self.sconv3(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training, p=p)\n",
    "            x = self.classifier1(x)\n",
    "            x = F.log_softmax(x,dim=1)\n",
    "            return x\n",
    "\n",
    "    class pure_sage(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.sconv1 = SAGEConv(4, 20)\n",
    "            self.resgatenn = SAGEConv(20, 20)\n",
    "            self.sconv2 = SAGEConv(20, 20)\n",
    "            self.sconv3 = SAGEConv(20, 20)\n",
    "            self.classifier1 = Linear(20,2)\n",
    "\n",
    "\n",
    "        def forward(self, data):\n",
    "            p = 0.1\n",
    "            x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "            x = self.sconv1(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training, p=p)\n",
    "            x = self.resgatenn(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training, p=p)\n",
    "            x = self.sconv2(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training, p=p)\n",
    "            x = self.sconv3(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training, p=p)\n",
    "            x = self.classifier1(x)\n",
    "            x = F.log_softmax(x,dim=1)\n",
    "            return x\n",
    "\n",
    "    class graph_conv(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.sconv1 = GraphConv(4, 20)\n",
    "            self.resgatenn = GraphConv(20, 20)\n",
    "            self.sconv2 = GraphConv(20, 20)\n",
    "            self.sconv3 = GraphConv(20, 20)\n",
    "            self.classifier1 = Linear(20,2)\n",
    "\n",
    "\n",
    "        def forward(self, data):\n",
    "            p = 0.1\n",
    "            x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "            x = self.sconv1(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training, p=p)\n",
    "            x = self.resgatenn(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training, p=p)\n",
    "            x = self.sconv2(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training, p=p)\n",
    "            x = self.sconv3(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training, p=p)\n",
    "            x = self.classifier1(x)\n",
    "            x = F.log_softmax(x,dim=1)\n",
    "            return x\n",
    "\n",
    "    class chebby_net(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            k = 3\n",
    "            self.sconv1 = ChebConv(4, 20,k)\n",
    "            self.resgatenn = ChebConv(20, 20,k)\n",
    "            self.sconv2 = ChebConv(20, 20,k)\n",
    "            self.sconv3 = ChebConv(20, 20,k)\n",
    "            self.classifier1 = Linear(20,2)\n",
    "\n",
    "\n",
    "        def forward(self, data):\n",
    "            p = 0.1\n",
    "            x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "            x = self.sconv1(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training, p=p)\n",
    "            x = self.resgatenn(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training, p=p)\n",
    "            x = self.sconv2(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training, p=p)\n",
    "            x = self.sconv3(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training, p=p)\n",
    "            x = self.classifier1(x)\n",
    "            x = F.log_softmax(x)\n",
    "            return x\n",
    "\n",
    "    #define Net\n",
    "    class sage_res(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.sconv1 = SAGEConv(4, 20)\n",
    "            self.resgatenn = ResGatedGraphConv(20, 20)\n",
    "            self.sconv2 = SAGEConv(20, 20)\n",
    "            self.sconv3 = SAGEConv(20, 20)\n",
    "            self.classifier1 = Linear(20,2)\n",
    "\n",
    "\n",
    "        def forward(self, data):\n",
    "            p = 0.1\n",
    "            x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "            x = self.sconv1(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training, p=p)\n",
    "            x = self.resgatenn(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training, p=p)\n",
    "            x = self.sconv2(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training, p=p)\n",
    "            x = self.sconv3(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training, p=p)\n",
    "            x = self.classifier1(x)\n",
    "            x = F.log_softmax(x,dim=1)\n",
    "            return x\n",
    "    #define Net\n",
    "    class gcn(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.sconv1 = GCNConv(4, 20)\n",
    "            self.resgatenn = GCNConv(20, 20)\n",
    "            self.sconv2 = GCNConv(20, 20)\n",
    "            self.sconv3 = GCNConv(20, 20)\n",
    "            self.classifier1 = Linear(20,2)\n",
    "\n",
    "\n",
    "        def forward(self, data):\n",
    "            p = 0.1\n",
    "            x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "            x = self.sconv1(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training, p=p)\n",
    "            x = self.resgatenn(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training, p=p)\n",
    "            x = self.sconv2(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training, p=p)\n",
    "            x = self.sconv3(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training, p=p)\n",
    "            x = self.classifier1(x)\n",
    "            x = F.log_softmax(x,dim=1)\n",
    "            return x\n",
    "\n",
    "    class Net1(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.sconv1 = ChebConv(1, 20,3)\n",
    "            self.resgatenn = ChebConv(20, 20,3)\n",
    "            self.sconv2 = ResGatedGraphConv(20, 20)\n",
    "            self.sconv3 = ResGatedGraphConv(20, 20)\n",
    "            self.classifier1 = Linear(20,2)\n",
    "\n",
    "\n",
    "        def forward(self, data):\n",
    "            p = 0.1\n",
    "            x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "            x = self.sconv1(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training, p=p)\n",
    "            x = self.resgatenn(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training, p=p)\n",
    "            x = self.sconv2(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training, p=p)\n",
    "            x = self.sconv3(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training, p=p)\n",
    "            x = self.classifier1(x)\n",
    "            x = F.log_softmax(x,dim=1)\n",
    "            return x\n",
    "\n",
    "    #define Net\n",
    "    class simple_u(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            normalize = True\n",
    "            self.sagec1 = SAGEConv(4,20, normalize=normalize) #[20,200]\n",
    "            self.sagp1 = SAGPooling(20)\n",
    "\n",
    "            self.sagec2 = SAGEConv(20,40, normalize=normalize)\n",
    "\n",
    "            self.sagec3 = SAGEConv(20,20, normalize=normalize)\n",
    "            self.classifier = Linear(20,2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        def forward(self, data):\n",
    "            p = 0.1\n",
    "            x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "            x = self.sagec1(x, edge_index) #first layer sage\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=p)\n",
    "\n",
    "\n",
    "            x1, edge_index1, weights1, batch1, perm,_ = self.sagp1(x, edge_index,batch=batch)\n",
    "            x1 = self.sagec2(x1,edge_index1) #simple u-net\n",
    "            x1 = F.relu(x1)\n",
    "            x1 = F.dropout(x1, p=p)\n",
    "            x = x+x1.reshape(x.shape) #concetenate U with first layer\n",
    "            x = self.sagec3(x,edge_index) #third layer sage\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=p)\n",
    "\n",
    "            x = self.classifier(x)\n",
    "            x = F.log_softmax(x,dim=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            return x\n",
    "\n",
    "\n",
    "    #define Net\n",
    "    class u_net(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            normalize = True\n",
    "            size = 20\n",
    "            self.sagec1 = SAGEConv(4,size, normalize=normalize) #[20,200]\n",
    "            self.sagp1 = SAGPooling(size)\n",
    "            self.bn1 = BatchNorm(size)\n",
    "\n",
    "            self.sagec2 = SAGEConv(size,size, normalize=normalize)\n",
    "            self.sagp2 = SAGPooling(size)\n",
    "            self.bn2 = BatchNorm(size)\n",
    "\n",
    "            self.sagec3 = SAGEConv(size,size, normalize=normalize)\n",
    "            self.sagp3 = SAGPooling(size)\n",
    "            self.bn3 = BatchNorm(size)\n",
    "\n",
    "            self.up1 = SAGEConv(size,size*2, normalize=normalize)\n",
    "\n",
    "            self.up2 = SAGEConv(size,size*2, normalize=normalize)\n",
    "\n",
    "            self.sagec7 = SAGEConv(size,size, normalize=normalize)\n",
    "            self.classifier = Linear(size,2)\n",
    "\n",
    "\n",
    "\n",
    "        def forward(self, data):\n",
    "            p = 0.1\n",
    "            x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "            x = self.sagec1(x, edge_index) #first layer sage\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=p)\n",
    "            #first downsampling layer\n",
    "            x1, edge_index1, weights1, batch1, perm1,_ = self.sagp1(x, edge_index, batch=batch)\n",
    "            x1 = self.sagec2(x1,edge_index1) #simple u-net\n",
    "            x1 = self.bn1(x1)\n",
    "            x1 = F.relu(x1)\n",
    "            x1 = F.dropout(x1, p=p)\n",
    "            #second downsampling layer\n",
    "            x2, edge_index2, weights2, batch2, perm2,_ = self.sagp1(x1, edge_index1, batch=batch1)\n",
    "            x2 = self.sagec2(x2,edge_index2) #simple u-net\n",
    "            x2 = self.bn2(x2)\n",
    "            x2 = F.relu(x2)\n",
    "            x2 = F.dropout(x2, p=p)\n",
    "            #first upscaling layer\n",
    "            x2 = self.up1(x2, edge_index2)\n",
    "            x2 = F.relu(x2)\n",
    "            x1 = x1+x2.reshape(x1.shape)\n",
    "            x1 = F.dropout(x1, p=p)\n",
    "            #second upscaling layer\n",
    "            x1 = self.up2(x1, edge_index1)\n",
    "            x1 = F.relu(x1)\n",
    "            x = x+x1.reshape(x.shape)\n",
    "            x = F.dropout(x, p=p)\n",
    "            #last concetenated layer\n",
    "            x = self.sagec3(x,edge_index) #third layer sage\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=p)\n",
    "            #classification layer                 \n",
    "            x = self.classifier(x)\n",
    "            x = F.log_softmax(x,dim=1)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4becb6b6-c98b-446d-b695-cbd71d2fc685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative implementation of pytorch_geometric.data.DataLoader with memory handling\n",
    "class classDataLoader:\n",
    "    def __init__(self, dataset, batchsize):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.batchsize = batchsize\n",
    "    \n",
    "    def __iter__(self):\n",
    "        self.index=0\n",
    "        self.batch = DataLoader(self.dataset, self.batchsize)\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.index >= batchsize-1:\n",
    "            raise StopIteration\n",
    "        return self.batch[self.index]\n",
    "\n",
    "    def batch(self):\n",
    "        return DataLoader(self.dataset, self.batchsize)\n",
    "    \n",
    " # \"backend\" implementation of DataLoader to create actual batches\n",
    "def DataLoader(dataset, batchsize):\n",
    "    batch_list = []\n",
    "    bat = Batch()\n",
    "    np.random.shuffle(dataset)\n",
    "    \n",
    "    N = len(dataset)\n",
    "    full_batches = int(N/batchsize)\n",
    "    extra = full_batches*batchsize\n",
    "    \n",
    "    for batch_number in range(full_batches):\n",
    "        bat = Batch()\n",
    "        data_list = dataset[batchsize*batch_number:batchsize*(batch_number+1)]\n",
    "        batch_list.append(bat.from_data_list(data_list))\n",
    "    \n",
    "    if extra!=N:\n",
    "        extra_data_list = dataset[extra:]\n",
    "        batch_list.append(bat.from_data_list(extra_data_list))\n",
    "    \n",
    "    return batch_list\n",
    "\n",
    "# returns dataset from list of batches (dtype Batch -> dtype Data)\n",
    "\n",
    "def DataUnLoader(batch_list):\n",
    "    dataset = []\n",
    "    batch_amount = len(batch_list)\n",
    "    \n",
    "    for batch_index in range(batch_amount):\n",
    "        dataset = dataset + batch_list[batch_index].to_data_list()\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ba8f602-f1bd-4995-9dd6-ab41adc1aafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-03 16:36:38,880\tINFO services.py:1263 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "\u001b[2m\u001b[36m(pid=7744)\u001b[0m 2021-10-03 16:36:53,989\tERROR serialization.py:256 -- [WinError 1455] The paging file is too small for this operation to complete. Error loading \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\torch\\lib\\caffe2_detectron_ops_gpu.dll\" or one of its dependencies.\n",
      "\u001b[2m\u001b[36m(pid=7744)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=7744)\u001b[0m   File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\ray\\serialization.py\", line 254, in deserialize_objects\n",
      "\u001b[2m\u001b[36m(pid=7744)\u001b[0m     obj = self._deserialize_object(data, metadata, object_ref)\n",
      "\u001b[2m\u001b[36m(pid=7744)\u001b[0m   File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\ray\\serialization.py\", line 190, in _deserialize_object\n",
      "\u001b[2m\u001b[36m(pid=7744)\u001b[0m     return self._deserialize_msgpack_data(data, metadata_fields)\n",
      "\u001b[2m\u001b[36m(pid=7744)\u001b[0m   File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\ray\\serialization.py\", line 168, in _deserialize_msgpack_data\n",
      "\u001b[2m\u001b[36m(pid=7744)\u001b[0m     python_objects = self._deserialize_pickle5_data(pickle5_data)\n",
      "\u001b[2m\u001b[36m(pid=7744)\u001b[0m   File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\ray\\serialization.py\", line 158, in _deserialize_pickle5_data\n",
      "\u001b[2m\u001b[36m(pid=7744)\u001b[0m     obj = pickle.loads(in_band)\n",
      "\u001b[2m\u001b[36m(pid=7744)\u001b[0m   File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\ray\\cloudpickle\\cloudpickle.py\", line 584, in subimport\n",
      "\u001b[2m\u001b[36m(pid=7744)\u001b[0m     __import__(name)\n",
      "\u001b[2m\u001b[36m(pid=7744)\u001b[0m   File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\torch\\__init__.py\", line 124, in <module>\n",
      "\u001b[2m\u001b[36m(pid=7744)\u001b[0m     raise err\n",
      "\u001b[2m\u001b[36m(pid=7744)\u001b[0m OSError: [WinError 1455] The paging file is too small for this operation to complete. Error loading \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\torch\\lib\\caffe2_detectron_ops_gpu.dll\" or one of its dependencies.\n",
      "\u001b[2m\u001b[36m(pid=33104)\u001b[0m 2021-10-03 16:36:54,020\tERROR serialization.py:256 -- [WinError 1455] The paging file is too small for this operation to complete. Error loading \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\torch\\lib\\caffe2_detectron_ops_gpu.dll\" or one of its dependencies.\n",
      "\u001b[2m\u001b[36m(pid=33104)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=33104)\u001b[0m   File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\ray\\serialization.py\", line 254, in deserialize_objects\n",
      "\u001b[2m\u001b[36m(pid=33104)\u001b[0m     obj = self._deserialize_object(data, metadata, object_ref)\n",
      "\u001b[2m\u001b[36m(pid=33104)\u001b[0m   File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\ray\\serialization.py\", line 190, in _deserialize_object\n",
      "\u001b[2m\u001b[36m(pid=33104)\u001b[0m     return self._deserialize_msgpack_data(data, metadata_fields)\n",
      "\u001b[2m\u001b[36m(pid=33104)\u001b[0m   File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\ray\\serialization.py\", line 168, in _deserialize_msgpack_data\n",
      "\u001b[2m\u001b[36m(pid=33104)\u001b[0m     python_objects = self._deserialize_pickle5_data(pickle5_data)\n",
      "\u001b[2m\u001b[36m(pid=33104)\u001b[0m   File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\ray\\serialization.py\", line 158, in _deserialize_pickle5_data\n",
      "\u001b[2m\u001b[36m(pid=33104)\u001b[0m     obj = pickle.loads(in_band)\n",
      "\u001b[2m\u001b[36m(pid=33104)\u001b[0m   File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\ray\\cloudpickle\\cloudpickle.py\", line 584, in subimport\n",
      "\u001b[2m\u001b[36m(pid=33104)\u001b[0m     __import__(name)\n",
      "\u001b[2m\u001b[36m(pid=33104)\u001b[0m   File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\torch\\__init__.py\", line 124, in <module>\n",
      "\u001b[2m\u001b[36m(pid=33104)\u001b[0m     raise err\n",
      "\u001b[2m\u001b[36m(pid=33104)\u001b[0m OSError: [WinError 1455] The paging file is too small for this operation to complete. Error loading \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\torch\\lib\\caffe2_detectron_ops_gpu.dll\" or one of its dependencies.\n"
     ]
    },
    {
     "ename": "RayTaskError",
     "evalue": "\u001b[36mray::SEIR_gen()\u001b[39m (pid=7744, ip=10.16.4.194)\n  File \"python\\ray\\_raylet.pyx\", line 495, in ray._raylet.execute_task\n  File \"python\\ray\\_raylet.pyx\", line 516, in ray._raylet.execute_task\n  File \"python\\ray\\_raylet.pyx\", line 385, in ray._raylet.raise_if_dependency_failed\nray.exceptions.RaySystemError: System error: [WinError 1455] The paging file is too small for this operation to complete. Error loading \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\torch\\lib\\caffe2_detectron_ops_gpu.dll\" or one of its dependencies.\ntraceback: Traceback (most recent call last):\n  File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\ray\\serialization.py\", line 254, in deserialize_objects\n    obj = self._deserialize_object(data, metadata, object_ref)\n  File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\ray\\serialization.py\", line 190, in _deserialize_object\n    return self._deserialize_msgpack_data(data, metadata_fields)\n  File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\ray\\serialization.py\", line 168, in _deserialize_msgpack_data\n    python_objects = self._deserialize_pickle5_data(pickle5_data)\n  File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\ray\\serialization.py\", line 158, in _deserialize_pickle5_data\n    obj = pickle.loads(in_band)\n  File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\ray\\cloudpickle\\cloudpickle.py\", line 584, in subimport\n    __import__(name)\n  File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\torch\\__init__.py\", line 124, in <module>\n    raise err\nOSError: [WinError 1455] The paging file is too small for this operation to complete. Error loading \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\torch\\lib\\caffe2_detectron_ops_gpu.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRayTaskError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4668/2909845567.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4668/3281446.py\u001b[0m in \u001b[0;36mgenerator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     30\u001b[0m                 show_progress = self.show_progress, types=self.types,save=self.save)\n\u001b[0;32m     31\u001b[0m         \u001b[1;31m#generate data given the parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0msims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4668/863624088.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mxs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSEIR_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremote\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCPU_threads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m         \u001b[0mxs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m         \u001b[0mray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data generated.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mclient_mode_should_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env\\lib\\site-packages\\ray\\worker.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(object_refs, timeout)\u001b[0m\n\u001b[0;32m   1619\u001b[0m                     \u001b[0mworker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore_worker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump_object_store_memory_usage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1620\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRayTaskError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1621\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_instanceof_cause\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1622\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1623\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRayTaskError\u001b[0m: \u001b[36mray::SEIR_gen()\u001b[39m (pid=7744, ip=10.16.4.194)\n  File \"python\\ray\\_raylet.pyx\", line 495, in ray._raylet.execute_task\n  File \"python\\ray\\_raylet.pyx\", line 516, in ray._raylet.execute_task\n  File \"python\\ray\\_raylet.pyx\", line 385, in ray._raylet.raise_if_dependency_failed\nray.exceptions.RaySystemError: System error: [WinError 1455] The paging file is too small for this operation to complete. Error loading \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\torch\\lib\\caffe2_detectron_ops_gpu.dll\" or one of its dependencies.\ntraceback: Traceback (most recent call last):\n  File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\ray\\serialization.py\", line 254, in deserialize_objects\n    obj = self._deserialize_object(data, metadata, object_ref)\n  File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\ray\\serialization.py\", line 190, in _deserialize_object\n    return self._deserialize_msgpack_data(data, metadata_fields)\n  File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\ray\\serialization.py\", line 168, in _deserialize_msgpack_data\n    python_objects = self._deserialize_pickle5_data(pickle5_data)\n  File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\ray\\serialization.py\", line 158, in _deserialize_pickle5_data\n    obj = pickle.loads(in_band)\n  File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\ray\\cloudpickle\\cloudpickle.py\", line 584, in subimport\n    __import__(name)\n  File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\torch\\__init__.py\", line 124, in <module>\n    raise err\nOSError: [WinError 1455] The paging file is too small for this operation to complete. Error loading \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\torch\\lib\\caffe2_detectron_ops_gpu.dll\" or one of its dependencies."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=488)\u001b[0m \u001b[1mExecuting function generate_data :\n",
      "\u001b[2m\u001b[36m(pid=488)\u001b[0m \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=9820)\u001b[0m 2021-10-03 16:36:56,618\tERROR serialization.py:256 -- module 'torch' has no attribute '_utils_internal'\n",
      "\u001b[2m\u001b[36m(pid=9820)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=9820)\u001b[0m   File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\ray\\serialization.py\", line 254, in deserialize_objects\n",
      "\u001b[2m\u001b[36m(pid=9820)\u001b[0m     obj = self._deserialize_object(data, metadata, object_ref)\n",
      "\u001b[2m\u001b[36m(pid=9820)\u001b[0m   File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\ray\\serialization.py\", line 190, in _deserialize_object\n",
      "\u001b[2m\u001b[36m(pid=9820)\u001b[0m     return self._deserialize_msgpack_data(data, metadata_fields)\n",
      "\u001b[2m\u001b[36m(pid=9820)\u001b[0m   File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\ray\\serialization.py\", line 168, in _deserialize_msgpack_data\n",
      "\u001b[2m\u001b[36m(pid=9820)\u001b[0m     python_objects = self._deserialize_pickle5_data(pickle5_data)\n",
      "\u001b[2m\u001b[36m(pid=9820)\u001b[0m   File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\ray\\serialization.py\", line 158, in _deserialize_pickle5_data\n",
      "\u001b[2m\u001b[36m(pid=9820)\u001b[0m     obj = pickle.loads(in_band)\n",
      "\u001b[2m\u001b[36m(pid=9820)\u001b[0m   File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\torch_geometric\\__init__.py\", line 5, in <module>\n",
      "\u001b[2m\u001b[36m(pid=9820)\u001b[0m     import torch_geometric.data\n",
      "\u001b[2m\u001b[36m(pid=9820)\u001b[0m   File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\torch_geometric\\data\\__init__.py\", line 1, in <module>\n",
      "\u001b[2m\u001b[36m(pid=9820)\u001b[0m     from .data import Data\n",
      "\u001b[2m\u001b[36m(pid=9820)\u001b[0m   File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\torch_geometric\\data\\data.py\", line 8, in <module>\n",
      "\u001b[2m\u001b[36m(pid=9820)\u001b[0m     from torch_sparse import coalesce, SparseTensor\n",
      "\u001b[2m\u001b[36m(pid=9820)\u001b[0m   File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\torch_sparse\\__init__.py\", line 14, in <module>\n",
      "\u001b[2m\u001b[36m(pid=9820)\u001b[0m     torch.ops.load_library(importlib.machinery.PathFinder().find_spec(\n",
      "\u001b[2m\u001b[36m(pid=9820)\u001b[0m   File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\torch\\_ops.py\", line 99, in load_library\n",
      "\u001b[2m\u001b[36m(pid=9820)\u001b[0m     path = torch._utils_internal.resolve_library_path(path)\n",
      "\u001b[2m\u001b[36m(pid=9820)\u001b[0m AttributeError: module 'torch' has no attribute '_utils_internal'\n",
      "\u001b[2m\u001b[36m(pid=16168)\u001b[0m 2021-10-03 16:36:57,387\tERROR serialization.py:256 -- module 'torch' has no attribute '_utils_internal'\n",
      "\u001b[2m\u001b[36m(pid=16168)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=16168)\u001b[0m   File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\ray\\serialization.py\", line 254, in deserialize_objects\n",
      "\u001b[2m\u001b[36m(pid=16168)\u001b[0m     obj = self._deserialize_object(data, metadata, object_ref)\n",
      "\u001b[2m\u001b[36m(pid=16168)\u001b[0m   File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\ray\\serialization.py\", line 190, in _deserialize_object\n",
      "\u001b[2m\u001b[36m(pid=16168)\u001b[0m     return self._deserialize_msgpack_data(data, metadata_fields)\n",
      "\u001b[2m\u001b[36m(pid=16168)\u001b[0m   File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\ray\\serialization.py\", line 168, in _deserialize_msgpack_data\n",
      "\u001b[2m\u001b[36m(pid=16168)\u001b[0m     python_objects = self._deserialize_pickle5_data(pickle5_data)\n",
      "\u001b[2m\u001b[36m(pid=16168)\u001b[0m   File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\ray\\serialization.py\", line 158, in _deserialize_pickle5_data\n",
      "\u001b[2m\u001b[36m(pid=16168)\u001b[0m     obj = pickle.loads(in_band)\n",
      "\u001b[2m\u001b[36m(pid=16168)\u001b[0m   File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\torch_geometric\\__init__.py\", line 5, in <module>\n",
      "\u001b[2m\u001b[36m(pid=16168)\u001b[0m     import torch_geometric.data\n",
      "\u001b[2m\u001b[36m(pid=16168)\u001b[0m   File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\torch_geometric\\data\\__init__.py\", line 1, in <module>\n",
      "\u001b[2m\u001b[36m(pid=16168)\u001b[0m     from .data import Data\n",
      "\u001b[2m\u001b[36m(pid=16168)\u001b[0m   File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\torch_geometric\\data\\data.py\", line 8, in <module>\n",
      "\u001b[2m\u001b[36m(pid=16168)\u001b[0m     from torch_sparse import coalesce, SparseTensor\n",
      "\u001b[2m\u001b[36m(pid=16168)\u001b[0m   File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\torch_sparse\\__init__.py\", line 14, in <module>\n",
      "\u001b[2m\u001b[36m(pid=16168)\u001b[0m     torch.ops.load_library(importlib.machinery.PathFinder().find_spec(\n",
      "\u001b[2m\u001b[36m(pid=16168)\u001b[0m   File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\torch\\_ops.py\", line 99, in load_library\n",
      "\u001b[2m\u001b[36m(pid=16168)\u001b[0m     path = torch._utils_internal.resolve_library_path(path)\n",
      "\u001b[2m\u001b[36m(pid=16168)\u001b[0m AttributeError: module 'torch' has no attribute '_utils_internal'\n",
      "\u001b[2m\u001b[36m(pid=34904)\u001b[0m 2021-10-03 16:36:59,425\tERROR serialization.py:256 -- module 'torch' has no attribute '_utils_internal'\n",
      "\u001b[2m\u001b[36m(pid=34904)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=34904)\u001b[0m   File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\ray\\serialization.py\", line 254, in deserialize_objects\n",
      "\u001b[2m\u001b[36m(pid=34904)\u001b[0m     obj = self._deserialize_object(data, metadata, object_ref)\n",
      "\u001b[2m\u001b[36m(pid=34904)\u001b[0m   File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\ray\\serialization.py\", line 190, in _deserialize_object\n",
      "\u001b[2m\u001b[36m(pid=34904)\u001b[0m     return self._deserialize_msgpack_data(data, metadata_fields)\n",
      "\u001b[2m\u001b[36m(pid=34904)\u001b[0m   File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\ray\\serialization.py\", line 168, in _deserialize_msgpack_data\n",
      "\u001b[2m\u001b[36m(pid=34904)\u001b[0m     python_objects = self._deserialize_pickle5_data(pickle5_data)\n",
      "\u001b[2m\u001b[36m(pid=34904)\u001b[0m   File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\ray\\serialization.py\", line 158, in _deserialize_pickle5_data\n",
      "\u001b[2m\u001b[36m(pid=34904)\u001b[0m     obj = pickle.loads(in_band)\n",
      "\u001b[2m\u001b[36m(pid=34904)\u001b[0m   File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\torch_geometric\\__init__.py\", line 5, in <module>\n",
      "\u001b[2m\u001b[36m(pid=34904)\u001b[0m     import torch_geometric.data\n",
      "\u001b[2m\u001b[36m(pid=34904)\u001b[0m   File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\torch_geometric\\data\\__init__.py\", line 1, in <module>\n",
      "\u001b[2m\u001b[36m(pid=34904)\u001b[0m     from .data import Data\n",
      "\u001b[2m\u001b[36m(pid=34904)\u001b[0m   File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\torch_geometric\\data\\data.py\", line 8, in <module>\n",
      "\u001b[2m\u001b[36m(pid=34904)\u001b[0m     from torch_sparse import coalesce, SparseTensor\n",
      "\u001b[2m\u001b[36m(pid=34904)\u001b[0m   File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\torch_sparse\\__init__.py\", line 14, in <module>\n",
      "\u001b[2m\u001b[36m(pid=34904)\u001b[0m     torch.ops.load_library(importlib.machinery.PathFinder().find_spec(\n",
      "\u001b[2m\u001b[36m(pid=34904)\u001b[0m   File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\torch\\_ops.py\", line 99, in load_library\n",
      "\u001b[2m\u001b[36m(pid=34904)\u001b[0m     path = torch._utils_internal.resolve_library_path(path)\n",
      "\u001b[2m\u001b[36m(pid=34904)\u001b[0m AttributeError: module 'torch' has no attribute '_utils_internal'\n",
      "\u001b[2m\u001b[36m(pid=14164)\u001b[0m 2021-10-03 16:36:59,619\tERROR serialization.py:256 -- module 'torch' has no attribute '_utils_internal'\n",
      "\u001b[2m\u001b[36m(pid=14164)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=14164)\u001b[0m   File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\ray\\serialization.py\", line 254, in deserialize_objects\n",
      "\u001b[2m\u001b[36m(pid=14164)\u001b[0m     obj = self._deserialize_object(data, metadata, object_ref)\n",
      "\u001b[2m\u001b[36m(pid=14164)\u001b[0m   File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\ray\\serialization.py\", line 190, in _deserialize_object\n",
      "\u001b[2m\u001b[36m(pid=14164)\u001b[0m     return self._deserialize_msgpack_data(data, metadata_fields)\n",
      "\u001b[2m\u001b[36m(pid=14164)\u001b[0m   File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\ray\\serialization.py\", line 168, in _deserialize_msgpack_data\n",
      "\u001b[2m\u001b[36m(pid=14164)\u001b[0m     python_objects = self._deserialize_pickle5_data(pickle5_data)\n",
      "\u001b[2m\u001b[36m(pid=14164)\u001b[0m   File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\ray\\serialization.py\", line 158, in _deserialize_pickle5_data\n",
      "\u001b[2m\u001b[36m(pid=14164)\u001b[0m     obj = pickle.loads(in_band)\n",
      "\u001b[2m\u001b[36m(pid=14164)\u001b[0m   File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\torch_geometric\\__init__.py\", line 5, in <module>\n",
      "\u001b[2m\u001b[36m(pid=14164)\u001b[0m     import torch_geometric.data\n",
      "\u001b[2m\u001b[36m(pid=14164)\u001b[0m   File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\torch_geometric\\data\\__init__.py\", line 1, in <module>\n",
      "\u001b[2m\u001b[36m(pid=14164)\u001b[0m     from .data import Data\n",
      "\u001b[2m\u001b[36m(pid=14164)\u001b[0m   File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\torch_geometric\\data\\data.py\", line 8, in <module>\n",
      "\u001b[2m\u001b[36m(pid=14164)\u001b[0m     from torch_sparse import coalesce, SparseTensor\n",
      "\u001b[2m\u001b[36m(pid=14164)\u001b[0m   File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\torch_sparse\\__init__.py\", line 14, in <module>\n",
      "\u001b[2m\u001b[36m(pid=14164)\u001b[0m     torch.ops.load_library(importlib.machinery.PathFinder().find_spec(\n",
      "\u001b[2m\u001b[36m(pid=14164)\u001b[0m   File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\torch\\_ops.py\", line 99, in load_library\n",
      "\u001b[2m\u001b[36m(pid=14164)\u001b[0m     path = torch._utils_internal.resolve_library_path(path)\n",
      "\u001b[2m\u001b[36m(pid=14164)\u001b[0m AttributeError: module 'torch' has no attribute '_utils_internal'\n",
      "2021-10-03 16:37:00,321\tERROR worker.py:79 -- Unhandled error (suppress with RAY_IGNORE_UNHANDLED_ERRORS=1): \u001b[36mray::SEIR_gen()\u001b[39m (pid=33104, ip=10.16.4.194)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 495, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 516, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 385, in ray._raylet.raise_if_dependency_failed\n",
      "ray.exceptions.RaySystemError: System error: [WinError 1455] The paging file is too small for this operation to complete. Error loading \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\torch\\lib\\caffe2_detectron_ops_gpu.dll\" or one of its dependencies.\n",
      "traceback: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\ray\\serialization.py\", line 254, in deserialize_objects\n",
      "    obj = self._deserialize_object(data, metadata, object_ref)\n",
      "  File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\ray\\serialization.py\", line 190, in _deserialize_object\n",
      "    return self._deserialize_msgpack_data(data, metadata_fields)\n",
      "  File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\ray\\serialization.py\", line 168, in _deserialize_msgpack_data\n",
      "    python_objects = self._deserialize_pickle5_data(pickle5_data)\n",
      "  File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\ray\\serialization.py\", line 158, in _deserialize_pickle5_data\n",
      "    obj = pickle.loads(in_band)\n",
      "  File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\ray\\cloudpickle\\cloudpickle.py\", line 584, in subimport\n",
      "    __import__(name)\n",
      "  File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\torch\\__init__.py\", line 124, in <module>\n",
      "    raise err\n",
      "OSError: [WinError 1455] The paging file is too small for this operation to complete. Error loading \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\torch\\lib\\caffe2_detectron_ops_gpu.dll\" or one of its dependencies.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=40148)\u001b[0m \u001b[1mExecuting function generate_data :\n",
      "\u001b[2m\u001b[36m(pid=40148)\u001b[0m \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-03 16:37:01,940\tERROR worker.py:79 -- Unhandled error (suppress with RAY_IGNORE_UNHANDLED_ERRORS=1): \u001b[36mray::SEIR_gen()\u001b[39m (pid=9820, ip=10.16.4.194)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 495, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 516, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 385, in ray._raylet.raise_if_dependency_failed\n",
      "ray.exceptions.RaySystemError: System error: module 'torch' has no attribute '_utils_internal'\n",
      "traceback: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\ray\\serialization.py\", line 254, in deserialize_objects\n",
      "    obj = self._deserialize_object(data, metadata, object_ref)\n",
      "  File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\ray\\serialization.py\", line 190, in _deserialize_object\n",
      "    return self._deserialize_msgpack_data(data, metadata_fields)\n",
      "  File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\ray\\serialization.py\", line 168, in _deserialize_msgpack_data\n",
      "    python_objects = self._deserialize_pickle5_data(pickle5_data)\n",
      "  File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\ray\\serialization.py\", line 158, in _deserialize_pickle5_data\n",
      "    obj = pickle.loads(in_band)\n",
      "  File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\torch_geometric\\__init__.py\", line 5, in <module>\n",
      "    import torch_geometric.data\n",
      "  File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\torch_geometric\\data\\__init__.py\", line 1, in <module>\n",
      "    from .data import Data\n",
      "  File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\torch_geometric\\data\\data.py\", line 8, in <module>\n",
      "    from torch_sparse import coalesce, SparseTensor\n",
      "  File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\torch_sparse\\__init__.py\", line 14, in <module>\n",
      "    torch.ops.load_library(importlib.machinery.PathFinder().find_spec(\n",
      "  File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\torch\\_ops.py\", line 99, in load_library\n",
      "    path = torch._utils_internal.resolve_library_path(path)\n",
      "AttributeError: module 'torch' has no attribute '_utils_internal'\n",
      "2021-10-03 16:37:02,941\tERROR worker.py:79 -- Unhandled error (suppress with RAY_IGNORE_UNHANDLED_ERRORS=1): \u001b[36mray::SEIR_gen()\u001b[39m (pid=16168, ip=10.16.4.194)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 495, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 516, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 385, in ray._raylet.raise_if_dependency_failed\n",
      "ray.exceptions.RaySystemError: System error: module 'torch' has no attribute '_utils_internal'\n",
      "traceback: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\ray\\serialization.py\", line 254, in deserialize_objects\n",
      "    obj = self._deserialize_object(data, metadata, object_ref)\n",
      "  File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\ray\\serialization.py\", line 190, in _deserialize_object\n",
      "    return self._deserialize_msgpack_data(data, metadata_fields)\n",
      "  File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\ray\\serialization.py\", line 168, in _deserialize_msgpack_data\n",
      "    python_objects = self._deserialize_pickle5_data(pickle5_data)\n",
      "  File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\ray\\serialization.py\", line 158, in _deserialize_pickle5_data\n",
      "    obj = pickle.loads(in_band)\n",
      "  File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\torch_geometric\\__init__.py\", line 5, in <module>\n",
      "    import torch_geometric.data\n",
      "  File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\torch_geometric\\data\\__init__.py\", line 1, in <module>\n",
      "    from .data import Data\n",
      "  File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\torch_geometric\\data\\data.py\", line 8, in <module>\n",
      "    from torch_sparse import coalesce, SparseTensor\n",
      "  File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\torch_sparse\\__init__.py\", line 14, in <module>\n",
      "    torch.ops.load_library(importlib.machinery.PathFinder().find_spec(\n",
      "  File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\torch\\_ops.py\", line 99, in load_library\n",
      "    path = torch._utils_internal.resolve_library_path(path)\n",
      "AttributeError: module 'torch' has no attribute '_utils_internal'\n",
      "2021-10-03 16:37:04,958\tERROR worker.py:79 -- Unhandled error (suppress with RAY_IGNORE_UNHANDLED_ERRORS=1): \u001b[36mray::SEIR_gen()\u001b[39m (pid=14164, ip=10.16.4.194)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 495, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 516, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 385, in ray._raylet.raise_if_dependency_failed\n",
      "ray.exceptions.RaySystemError: System error: module 'torch' has no attribute '_utils_internal'\n",
      "traceback: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\ray\\serialization.py\", line 254, in deserialize_objects\n",
      "    obj = self._deserialize_object(data, metadata, object_ref)\n",
      "  File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\ray\\serialization.py\", line 190, in _deserialize_object\n",
      "    return self._deserialize_msgpack_data(data, metadata_fields)\n",
      "  File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\ray\\serialization.py\", line 168, in _deserialize_msgpack_data\n",
      "    python_objects = self._deserialize_pickle5_data(pickle5_data)\n",
      "  File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\ray\\serialization.py\", line 158, in _deserialize_pickle5_data\n",
      "    obj = pickle.loads(in_band)\n",
      "  File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\torch_geometric\\__init__.py\", line 5, in <module>\n",
      "    import torch_geometric.data\n",
      "  File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\torch_geometric\\data\\__init__.py\", line 1, in <module>\n",
      "    from .data import Data\n",
      "  File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\torch_geometric\\data\\data.py\", line 8, in <module>\n",
      "    from torch_sparse import coalesce, SparseTensor\n",
      "  File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\torch_sparse\\__init__.py\", line 14, in <module>\n",
      "    torch.ops.load_library(importlib.machinery.PathFinder().find_spec(\n",
      "  File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\torch\\_ops.py\", line 99, in load_library\n",
      "    path = torch._utils_internal.resolve_library_path(path)\n",
      "AttributeError: module 'torch' has no attribute '_utils_internal'\n",
      "2021-10-03 16:37:04,973\tERROR worker.py:79 -- Unhandled error (suppress with RAY_IGNORE_UNHANDLED_ERRORS=1): \u001b[36mray::SEIR_gen()\u001b[39m (pid=34904, ip=10.16.4.194)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 495, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 516, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 385, in ray._raylet.raise_if_dependency_failed\n",
      "ray.exceptions.RaySystemError: System error: module 'torch' has no attribute '_utils_internal'\n",
      "traceback: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\ray\\serialization.py\", line 254, in deserialize_objects\n",
      "    obj = self._deserialize_object(data, metadata, object_ref)\n",
      "  File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\ray\\serialization.py\", line 190, in _deserialize_object\n",
      "    return self._deserialize_msgpack_data(data, metadata_fields)\n",
      "  File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\ray\\serialization.py\", line 168, in _deserialize_msgpack_data\n",
      "    python_objects = self._deserialize_pickle5_data(pickle5_data)\n",
      "  File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\ray\\serialization.py\", line 158, in _deserialize_pickle5_data\n",
      "    obj = pickle.loads(in_band)\n",
      "  File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\torch_geometric\\__init__.py\", line 5, in <module>\n",
      "    import torch_geometric.data\n",
      "  File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\torch_geometric\\data\\__init__.py\", line 1, in <module>\n",
      "    from .data import Data\n",
      "  File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\torch_geometric\\data\\data.py\", line 8, in <module>\n",
      "    from torch_sparse import coalesce, SparseTensor\n",
      "  File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\torch_sparse\\__init__.py\", line 14, in <module>\n",
      "    torch.ops.load_library(importlib.machinery.PathFinder().find_spec(\n",
      "  File \"C:\\Users\\basti\\anaconda3\\envs\\env\\lib\\site-packages\\torch\\_ops.py\", line 99, in load_library\n",
      "    path = torch._utils_internal.resolve_library_path(path)\n",
      "AttributeError: module 'torch' has no attribute '_utils_internal'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=488)\u001b[0m \u001b[1m\n",
      "\u001b[2m\u001b[36m(pid=488)\u001b[0m Finished execution of function generate_data in 113.44862 seconds.\n",
      "\u001b[2m\u001b[36m(pid=488)\u001b[0m \u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=40148)\u001b[0m \u001b[1m\n",
      "\u001b[2m\u001b[36m(pid=40148)\u001b[0m Finished execution of function generate_data in 113.15056 seconds.\n",
      "\u001b[2m\u001b[36m(pid=40148)\u001b[0m \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = model()\n",
    "model.generator()\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0d3afd-eaf8-4934-a552-2b33dc13a2c2",
   "metadata": {},
   "source": [
    "# Aditional Data Manipulation and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669284be-dab9-4008-8787-dbf3ce237253",
   "metadata": {},
   "outputs": [],
   "source": [
    "def x_to_one_hot(dataset, features=4):\n",
    "    for it, data in enumerate(dataset):\n",
    "        one_hot = torch.zeros((data.x.shape[0], features))\n",
    "        one_hot[np.arange(data.x.shape[0], dtype=int), data.x[:,0].long()]= 1\n",
    "        data.x = one_hot.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0ff0fdf-d8c3-4a1a-b9c5-101767a2543b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print histogram of one hot data distribution\n",
    "def datahist(dataset, SEIR=True):\n",
    "    susceptible = np.zeros(400)\n",
    "    infected = np.zeros(400)\n",
    "    removed = np.zeros(400)\n",
    "    \n",
    "    s, e, i, r = [], [], [], []\n",
    "    \n",
    "    if SEIR:\n",
    "        for data in dataset:\n",
    "            s.append(int(np.sum(data.x.cpu().numpy()[:,0])))\n",
    "            e.append(int(np.sum(data.x.cpu().numpy()[:,1])))\n",
    "            i.append(int(np.sum(data.x.cpu().numpy()[:,2])))\n",
    "            r.append(int(np.sum(data.x.cpu().numpy()[:,3])))\n",
    "        \n",
    "        plt.title(\"susceptible\")\n",
    "        plt.hist(s, bins=70)\n",
    "        #plt.savefig(\"WS1\")\n",
    "        plt.show()\n",
    "        plt.title(\"exposed\")\n",
    "        plt.hist(e, bins=70)\n",
    "        #plt.savefig(\"WS2\")\n",
    "        plt.show()\n",
    "        plt.title(\"infected\")\n",
    "        plt.hist(i, bins=70)\n",
    "        #plt.savefig(\"WS3\")\n",
    "        plt.show()\n",
    "        plt.title(\"removed\")\n",
    "        plt.hist(r, bins=70)\n",
    "        #plt.savefig(\"WS4\")\n",
    "        plt.show()\n",
    "    \n",
    "    else:\n",
    "        for data in dataset:\n",
    "            s.append(int(np.sum(data.x.cpu().numpy()[:,0])))\n",
    "            i.append(int(np.sum(data.x.cpu().numpy()[:,1])))\n",
    "            r.append(int(np.sum(data.x.cpu().numpy()[:,2])))\n",
    "        \n",
    "        plt.title(\"susceptible\")\n",
    "        plt.hist(s, bins=70)\n",
    "        #plt.savefig(\"er_sir1.jpg\")\n",
    "        plt.show()\n",
    "        plt.title(\"infected\")\n",
    "        plt.hist(i, bins=70)\n",
    "        #plt.savefig(\"er_sir2.jpg\")\n",
    "        plt.show()\n",
    "        plt.title(\"removed\")\n",
    "        plt.hist(r, bins=70)\n",
    "        #plt.savefig(\"er_sir3.jpg\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ee52096-4c69-432f-8f55-38a995eb9b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function can be used to remove graphs where only one node is not susceptible\n",
    "def onenodefilter(dataset, total=400):\n",
    "    newdataset = dataset\n",
    "    simple = []\n",
    "    \n",
    "    for i, data in enumerate(dataset):\n",
    "         if torch.sum(data.x[:,0]).item() == total-1:\n",
    "            simple.append(i)\n",
    "\n",
    "    for s in simple[::-1]:\n",
    "        del newdataset[s]\n",
    "    print(\"len: \", len(newdataset))\n",
    "    return newdataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bb8eac-0972-458d-a676-e9614b20b149",
   "metadata": {},
   "source": [
    "# DMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba2261d2-83ae-46fc-a068-66beceb69a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run DMP.ipynb\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b81417b-5c9a-4d54-a417-5ef6a1142186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mExecuting function generate_data :\n",
      "\u001b[0m\n",
      "0 %\n",
      "10 %\n",
      "20 %\n",
      "30 %\n",
      "40 %\n",
      "50 %\n",
      "60 %\n",
      "70 %\n",
      "80 %\n",
      "90 %\n",
      "\u001b[1m\n",
      "Finished execution of function generate_data in 0.39409 seconds.\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "batch_amount = 100\n",
    "# Generate Data using SEIRdata\n",
    "graph_model = SIRdata(batches = batch_amount,show_progress=True, graph_type = \"ER\", iterations = 50, N=50 ,p=6/100, beta=1/20, gamma=1/50)\n",
    "sirData = graph_model.generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a28a4257-a522-490e-b026-010ca98ae102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to dtype data\n",
    "sir_dataset = []\n",
    "for graph_index in range(batch_amount):\n",
    "    sir_dataset.append(Data(edge_index = sirData[graph_index][1], x = sirData[graph_index][2], y = sirData[graph_index][3]))\n",
    "    sir_dataset[-1] = sir_dataset[-1].to(device)\n",
    "del sirData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df36bea7-a22a-4280-8a46-8a1176e7c82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prog 0 %\n",
      "prog 1 %\n",
      "prog 2 %\n",
      "prog 3 %\n",
      "prog 4 %\n",
      "prog 5 %\n",
      "prog 6 %\n",
      "prog 7 %\n",
      "prog 8 %\n",
      "prog 9 %\n",
      "prog 10 %\n",
      "prog 11 %\n",
      "prog 12 %\n",
      "prog 13 %\n",
      "prog 14 %\n",
      "prog 15 %\n",
      "prog 16 %\n",
      "prog 17 %\n",
      "prog 18 %\n",
      "prog 19 %\n",
      "prog 20 %\n",
      "prog 21 %\n",
      "prog 22 %\n",
      "prog 23 %\n",
      "prog 24 %\n",
      "prog 25 %\n",
      "prog 26 %\n",
      "prog 27 %\n",
      "prog 28 %\n",
      "prog 29 %\n",
      "prog 30 %\n",
      "prog 31 %\n",
      "prog 32 %\n",
      "prog 33 %\n",
      "prog 34 %\n",
      "prog 35 %\n",
      "prog 36 %\n",
      "prog 37 %\n",
      "prog 38 %\n",
      "prog 39 %\n",
      "prog 40 %\n",
      "prog 41 %\n",
      "prog 42 %\n",
      "prog 43 %\n",
      "prog 44 %\n",
      "prog 45 %\n",
      "prog 46 %\n",
      "prog 47 %\n",
      "prog 48 %\n",
      "prog 49 %\n",
      "prog 50 %\n",
      "prog 51 %\n",
      "prog 52 %\n",
      "prog 53 %\n",
      "prog 54 %\n",
      "prog 55 %\n",
      "prog 56 %\n",
      "prog 57 %\n",
      "prog 58 %\n",
      "prog 59 %\n",
      "prog 60 %\n",
      "prog 61 %\n",
      "prog 62 %\n",
      "prog 63 %\n",
      "prog 64 %\n",
      "prog 65 %\n",
      "prog 66 %\n",
      "prog 67 %\n",
      "prog 68 %\n",
      "prog 69 %\n",
      "prog 70 %\n",
      "prog 71 %\n",
      "prog 72 %\n",
      "prog 73 %\n",
      "prog 74 %\n",
      "prog 75 %\n",
      "prog 76 %\n",
      "prog 77 %\n",
      "prog 78 %\n",
      "prog 79 %\n",
      "prog 80 %\n",
      "prog 81 %\n",
      "prog 82 %\n",
      "prog 83 %\n",
      "prog 84 %\n",
      "prog 85 %\n",
      "prog 86 %\n",
      "prog 87 %\n",
      "prog 88 %\n",
      "prog 89 %\n",
      "prog 90 %\n",
      "prog 91 %\n",
      "prog 92 %\n",
      "prog 93 %\n",
      "prog 94 %\n",
      "prog 95 %\n",
      "prog 96 %\n",
      "prog 97 %\n",
      "prog 98 %\n",
      "top10 accuracy:  75.0 %\n",
      "top5 accuracy:  61.0 %\n",
      "top1 accuracy:  28.999999999999996 %\n"
     ]
    }
   ],
   "source": [
    "# test dmp\n",
    "top10 = 0\n",
    "top5 = 0\n",
    "top1 = 0\n",
    "total = 0\n",
    "prog = 0\n",
    "\n",
    "tester = dmp_layer(lamb=1/40,mu=1/100) \n",
    "\n",
    "for it, data in enumerate(sir_dataset):\n",
    "    total +=1\n",
    "    edge_index = data.edge_index\n",
    "    snapshot = data.x\n",
    "    \n",
    "    prediction = tester.predict(edge_index, snapshot=snapshot, time_steps=100)\n",
    "    \n",
    "    correct = data.y.argmax().cpu().numpy()\n",
    "    if correct in prediction.argsort()[-10:]:\n",
    "        top10+=1\n",
    "        if correct in prediction.argsort()[-5:]:\n",
    "            top5+=1\n",
    "            if correct == prediction.argmax():\n",
    "                top1+=1\n",
    "\n",
    "    if it/len(sir_dataset)*100>prog:\n",
    "        print(\"prog\", prog, \"%\")\n",
    "        prog+=1\n",
    "print(\"top10 accuracy: \", top10/total*100, \"%\")\n",
    "print(\"top5 accuracy: \", top5/total*100, \"%\")\n",
    "print(\"top1 accuracy: \", top1/total*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76475670-4ba5-435d-87f7-1dc8edc975b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
